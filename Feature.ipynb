{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = r\"D:\\Major Project\\dataset\\deepfake-detection-challenge\\train_sample_videos\"\n",
    "test = r\"D:\\Major Project\\dataset\\deepfake-detection-challenge\\test_videos\"\n",
    "meta = r\"D:\\Major Project\\dataset\\deepfake-detection-challenge\\train_sample_videos\\metadata.json\"\n",
    "data = r\"D:\\Major Project\\dataset\\deepfake-detection-challenge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = pd.read_json(meta).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etejaapnxh.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wtreibcmgm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etmcruaihe.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>afoovlsmtx.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etohcvnzbj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>bdnaqemxmr.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eudeqjhdfd.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eukvucdetx.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>gjypopglvi.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4\n",
       "...              ...    ...             ...\n",
       "etejaapnxh.mp4  FAKE  train  wtreibcmgm.mp4\n",
       "etmcruaihe.mp4  FAKE  train  afoovlsmtx.mp4\n",
       "etohcvnzbj.mp4  FAKE  train  bdnaqemxmr.mp4\n",
       "eudeqjhdfd.mp4  REAL  train            None\n",
       "eukvucdetx.mp4  FAKE  train  gjypopglvi.mp4\n",
       "\n",
       "[400 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "batch_size = 64\n",
    "epochs = 15\n",
    "\n",
    "max_seq_length = 20\n",
    "num_features = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y,x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y :start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(img_size, img_size)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while 1:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "            \n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "    weights = \"imagenet\",\n",
    "    include_top=False,\n",
    "    pooling=\"avg\",\n",
    "    input_shape = (img_size,img_size,3)\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "    \n",
    "    inputs = keras.Input((img_size,img_size,3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    \n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = pretrain_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir): \n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(int)\n",
    "    \n",
    "    frame_masks = np.zeros(shape=(num_samples, max_seq_length), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, max_seq_length, num_features), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        \n",
    "        temp_frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "        \n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(max_seq_length, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] =feature_extractor.predict(batch[None, j, :])\n",
    "            temp_frame_mask[i, :length] =1 # 1 = not masked, 0 = masked\n",
    "        \n",
    "        frame_features[idx,] =temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] =temp_frame_mask.squeeze()\n",
    "    \n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set , Test_set = train_test_split(md, test_size=0.1,random_state=42,\n",
    "                                       stratify=md['label'])\n",
    "print(Train_set.shape, Test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = prepare_all_videos(Train_set, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(Test_set, \"test\")\n",
    "\n",
    "# print(f\"Frame features in train set:{train_data[0].shape}\")\n",
    "# print(f\"Frame masks in train set:{train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 20, 2048)]           0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " gru (GRU)                   (None, 20, 16)               99168     ['input_3[0][0]',             \n",
      "                                                                     'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                 (None, 8)                    624       ['gru[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 8)                    0         ['gru_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 8)                    72        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    9         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99873 (390.13 KB)\n",
      "Trainable params: 99873 (390.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frame_features_input = keras.Input((max_seq_length, num_features))\n",
    "mask_input = keras.Input((max_seq_length,),dtype=\"bool\")\n",
    "\n",
    "x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask = mask_input)\n",
    "x = keras.layers.GRU(8)(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model([frame_features_input, mask_input], output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "45/45 [==============================] - 7s 60ms/step - loss: 0.6871 - accuracy: 0.7917 - val_loss: 0.6816 - val_accuracy: 0.8000\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6756 - accuracy: 0.8083 - val_loss: 0.6700 - val_accuracy: 0.8000\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.6643 - accuracy: 0.8083 - val_loss: 0.6594 - val_accuracy: 0.8000\n",
      "Epoch 4/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6534 - accuracy: 0.8083 - val_loss: 0.6499 - val_accuracy: 0.8000\n",
      "Epoch 5/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.6434 - accuracy: 0.8083 - val_loss: 0.6398 - val_accuracy: 0.8000\n",
      "Epoch 6/15\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.6335 - accuracy: 0.8083 - val_loss: 0.6310 - val_accuracy: 0.8000\n",
      "Epoch 7/15\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 0.6245 - accuracy: 0.8083 - val_loss: 0.6222 - val_accuracy: 0.8000\n",
      "Epoch 8/15\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.6158 - accuracy: 0.8083 - val_loss: 0.6143 - val_accuracy: 0.8000\n",
      "Epoch 9/15\n",
      "45/45 [==============================] - 3s 60ms/step - loss: 0.6079 - accuracy: 0.8083 - val_loss: 0.6065 - val_accuracy: 0.8000\n",
      "Epoch 10/15\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.6001 - accuracy: 0.8083 - val_loss: 0.5994 - val_accuracy: 0.8000\n",
      "Epoch 11/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.5930 - accuracy: 0.8083 - val_loss: 0.5927 - val_accuracy: 0.8000\n",
      "Epoch 12/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.5861 - accuracy: 0.8083 - val_loss: 0.5866 - val_accuracy: 0.8000\n",
      "Epoch 13/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.5798 - accuracy: 0.8083 - val_loss: 0.5804 - val_accuracy: 0.8000\n",
      "Epoch 14/15\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.5738 - accuracy: 0.8083 - val_loss: 0.5749 - val_accuracy: 0.8000\n",
      "Epoch 15/15\n",
      "45/45 [==============================] - 1s 15ms/step - loss: 0.5682 - accuracy: 0.8083 - val_loss: 0.5696 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\n",
    "history = model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_data=([test_data[0], test_data[1]], test_labels),\n",
    "        callbacks=[checkpoint],\n",
    "        epochs=epochs,\n",
    "        batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos = pd.DataFrame(list(os.listdir(os.path.join(data, test))), columns=['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: orixbcfvdz.mp4\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "The predicted class of the video is FAKE\n"
     ]
    }
   ],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, max_seq_length,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, max_seq_length, num_features), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(max_seq_length, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    frames = load_video(os.path.join(data, test,path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    return model.predict([frame_features, frame_mask])[0]\n",
    "    \n",
    "# This utility is for visualization.\n",
    "# Referenced from:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "def to_gif(images):\n",
    "    converted_images = images.astype(np.uint8)\n",
    "    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n",
    "    return embed.embed_file(\"animation.gif\")\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "if(sequence_prediction(test_video)>=0.5):\n",
    "    print(f'The predicted class of the video is FAKE')\n",
    "else:\n",
    "    print(f'The predicted class of the video is REAL')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
